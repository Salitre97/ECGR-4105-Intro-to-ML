{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMOBrgO50RBtkkRuP/szrka",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Salitre97/ECGR-4105-Intro_to_ML/blob/cristian/Homework_5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "5fKK9RBGKivd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7e96a13-b33e-4dd3-d7e6-979d15615d0c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Problem 1"
      ],
      "metadata": {
        "id": "WCGMxBav2kGS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "t_c = [0.5, 14.0, 15.0, 28.0, 11.0, 8.0, 3.0, -4.0, 6.0, 13.0, 21.0]   # Temperature in Cesius\n",
        "t_u = [35.7, 55.9, 58.2, 81.9, 56.3, 48.9, 33.9, 21.8, 48.4, 60.4, 68.4]  # measurement\n",
        "t_c = torch.tensor(t_c)\n",
        "t_u = torch.tensor(t_u)"
      ],
      "metadata": {
        "id": "do46RsiLLsdu"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_samples = t_u.shape[0]\n",
        "n_val = int(0.2 * n_samples)\n",
        "\n",
        "shuffled_indices = torch.randperm(n_samples)\n",
        "\n",
        "train_indices = shuffled_indices[:-n_val]\n",
        "val_indices = shuffled_indices[-n_val:]\n",
        "\n",
        "train_indices, val_indices"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FFccTNtQPMeL",
        "outputId": "027b26ae-a91a-49e3-abc7-b1b85ef603fb"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([5, 9, 6, 2, 7, 3, 4, 0, 1]), tensor([ 8, 10]))"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_t_u = t_u[train_indices]\n",
        "train_t_c = t_c[train_indices]\n",
        "\n",
        "val_t_u = t_u[val_indices]\n",
        "val_t_c = t_c[val_indices]\n",
        "\n",
        "train_t_un = 0.1 * train_t_u\n",
        "val_t_un = 0.1 * val_t_u"
      ],
      "metadata": {
        "id": "rUh0fxXeQFFw"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Define non-linear model\n",
        "def nonlinear_model(t_u, w1, w2, b):\n",
        "    return w2*t_u ** 2 + w1*t_u + b\n",
        "\n",
        "def linear_model(t_u, w, b):\n",
        "    return w*t_u + b\n",
        "\n",
        "# loss function\n",
        "def loss_fn(t_p, t_c):\n",
        "    squared_diffs = (t_p - t_c)**2\n",
        "    return squared_diffs.mean()\n",
        "\n",
        "# Training loop for non-linear model\n",
        "def nonlinear_train(n_epochs, optimizer, params, train_t_u, val_t_u, train_t_c, val_t_c):\n",
        "    for epoch in range(1, n_epochs + 1):\n",
        "        train_t_p = nonlinear_model(train_t_u, *params)\n",
        "        train_loss = loss_fn(train_t_p, train_t_c)\n",
        "\n",
        "        val_t_p = nonlinear_model(val_t_u, *params)\n",
        "        val_loss = loss_fn(train_t_p, train_t_c)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        train_loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if epoch % 500 == 0:\n",
        "            print(f\"Epoch {epoch}, Training loss {train_loss.item():.4f},\" f\" validation loss {val_loss.item():.4f}\")\n",
        "\n",
        "    return params\n",
        "    print('\\n')\n",
        "# Training loop for linear model\n",
        "def linear_train(n_epochs, optimizer, params, train_t_u, val_t_u, train_t_c, val_t_c):\n",
        "    for epoch in range(1, n_epochs + 1):\n",
        "        train_t_p = linear_model(train_t_u, * params)\n",
        "        train_loss = loss_fn(train_t_p, train_t_c)\n",
        "\n",
        "        val_t_p = linear_model(val_t_u, *params)\n",
        "        val_loss = loss_fn(val_t_p, val_t_c)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        train_loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if epoch <= 3 or epoch % 500 == 0:\n",
        "            print(f\"Epoch {epoch}, Training loss {train_loss.item():.4f},\" f\" Validation loss {val_loss.item():.4f}\")\n",
        "    return params\n",
        "\n"
      ],
      "metadata": {
        "id": "bL5tn0KVICCq"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Different learning rates and optimizers\n",
        "learning_rates = [0.1, 0.01, 0.001, 0.0001]\n",
        "print('Stochastic Gradient Descent (SGD):\\n')\n",
        "for lr in learning_rates:\n",
        "  params = torch.tensor([1.0, 1.0, 0.0],  requires_grad=True)\n",
        "  optimizer = optim.SGD([params], lr=lr)\n",
        "  print(f'Learning rate: {lr}\\n')\n",
        "  nonlinear_train(n_epochs=5000, optimizer=optimizer, params=params, train_t_u=train_t_un, val_t_u=val_t_un, train_t_c=train_t_c, val_t_c=val_t_c)\n",
        "\n",
        "print('Adam Optimizer:\\n')\n",
        "for lr in learning_rates:\n",
        "  params = torch.tensor([1.0, 1.0, 0.0], requires_grad=True)\n",
        "  optimizer = optim.Adam([params], lr=lr)\n",
        "  print(f'Learning rate: {lr}\\n')\n",
        "  nonlinear_train(n_epochs=5000, optimizer=optimizer, params=params, train_t_u=train_t_un, val_t_u=val_t_un, train_t_c=train_t_c, val_t_c=val_t_c)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UFsJpfyiSDeu",
        "outputId": "743b89f9-3c6c-4ba2-9cf6-ca184e3fc644"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stochastic Gradient Descent (SGD):\n",
            "\n",
            "Learning rate: 0.1\n",
            "\n",
            "Epoch 500, Training loss nan, validation loss nan\n",
            "Epoch 1000, Training loss nan, validation loss nan\n",
            "Epoch 1500, Training loss nan, validation loss nan\n",
            "Epoch 2000, Training loss nan, validation loss nan\n",
            "Epoch 2500, Training loss nan, validation loss nan\n",
            "Epoch 3000, Training loss nan, validation loss nan\n",
            "Epoch 3500, Training loss nan, validation loss nan\n",
            "Epoch 4000, Training loss nan, validation loss nan\n",
            "Epoch 4500, Training loss nan, validation loss nan\n",
            "Epoch 5000, Training loss nan, validation loss nan\n",
            "Learning rate: 0.01\n",
            "\n",
            "Epoch 500, Training loss nan, validation loss nan\n",
            "Epoch 1000, Training loss nan, validation loss nan\n",
            "Epoch 1500, Training loss nan, validation loss nan\n",
            "Epoch 2000, Training loss nan, validation loss nan\n",
            "Epoch 2500, Training loss nan, validation loss nan\n",
            "Epoch 3000, Training loss nan, validation loss nan\n",
            "Epoch 3500, Training loss nan, validation loss nan\n",
            "Epoch 4000, Training loss nan, validation loss nan\n",
            "Epoch 4500, Training loss nan, validation loss nan\n",
            "Epoch 5000, Training loss nan, validation loss nan\n",
            "Learning rate: 0.001\n",
            "\n",
            "Epoch 500, Training loss nan, validation loss nan\n",
            "Epoch 1000, Training loss nan, validation loss nan\n",
            "Epoch 1500, Training loss nan, validation loss nan\n",
            "Epoch 2000, Training loss nan, validation loss nan\n",
            "Epoch 2500, Training loss nan, validation loss nan\n",
            "Epoch 3000, Training loss nan, validation loss nan\n",
            "Epoch 3500, Training loss nan, validation loss nan\n",
            "Epoch 4000, Training loss nan, validation loss nan\n",
            "Epoch 4500, Training loss nan, validation loss nan\n",
            "Epoch 5000, Training loss nan, validation loss nan\n",
            "Learning rate: 0.0001\n",
            "\n",
            "Epoch 500, Training loss 10.5338, validation loss 10.5338\n",
            "Epoch 1000, Training loss 8.4071, validation loss 8.4071\n",
            "Epoch 1500, Training loss 6.9466, validation loss 6.9466\n",
            "Epoch 2000, Training loss 5.9429, validation loss 5.9429\n",
            "Epoch 2500, Training loss 5.2526, validation loss 5.2526\n",
            "Epoch 3000, Training loss 4.7772, validation loss 4.7772\n",
            "Epoch 3500, Training loss 4.4492, validation loss 4.4492\n",
            "Epoch 4000, Training loss 4.2223, validation loss 4.2223\n",
            "Epoch 4500, Training loss 4.0648, validation loss 4.0648\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Best nonlinear model vs linear model (in class from textbook)\n",
        "nonlinear_params = torch.tensor([1.0, 1.0, 0.0], requires_grad=True)\n",
        "nonlinear_optimizer = optim.Adam([nonlinear_params], lr=0.1)\n",
        "linear_params = torch.tensor([1.0, 0.0], requires_grad=True)\n",
        "linear_optimizer = optim.Adam([nonlinear_params], lr=0.01)\n",
        "\n",
        "nonlinear = nonlinear_train(n_epochs=5000, optimizer=nonlinear_optimizer, params=nonlinear_params, train_t_u=train_t_un, val_t_u=val_t_un, train_t_c=train_t_c, val_t_c=val_t_c)\n",
        "linear = linear_train(n_epochs=5000, optimizer=linear_optimizer, params=linear_params, train_t_u=train_t_un, val_t_u=val_t_un, train_t_c=train_t_c, val_t_c=val_t_c )\n",
        "\n",
        "t_p_linear = linear_model(t_u, *linear).detach().numpy()\n",
        "t_p_nonlinear = nonlinear_model(t_u, *nonlinear).detach().numpy()\n",
        "\n",
        "plt.scatter(t_u, t_c, label='measurements')\n",
        "plt.plot(t_u, t_p_linear)\n",
        "plt.plot(t_u, t_p_nonlinear)\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "GMXfS94apYX1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Problem 2"
      ],
      "metadata": {
        "id": "ak0t8Wc82pWI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = '/content/drive/MyDrive/ML-4105/Housing.csv'\n",
        "housing = pd.DataFrame(pd.read_csv(file_path))\n",
        "housing.head(10)"
      ],
      "metadata": {
        "id": "y5_C8stP2qv2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model for 6 parameters\n",
        "def linear_model1(t_x, w5, w4, w3, w2, w1, b):\n",
        "    return torch.matmul(t_x, params[:-1].double()) + params[-1].double()\n",
        "\n",
        "# model for 11 parameters\n",
        "def linear_model2(t_x, w10, w9, w8, w7, w6, w5, w4, w3, w2, w1, b):\n",
        "    return torch.matmul(t_x, params[:-1].double()) + params[-1].double()\n",
        "\n",
        "def linear_train1(epochs, optimizer, params, train_t_x, val_t_x, train_t_y, val_t_y):\n",
        "  train_losses = []\n",
        "  val_losses = []\n",
        "\n",
        "  for epoch in range(1, epochs+1):\n",
        "    train_t_p = linear_model1(train_t_x, *params)\n",
        "    train_loss = loss_fn(train_t_p, train_t_y)\n",
        "\n",
        "    val_t_p = linear_model1(val_t_x, *params)\n",
        "    val_loss = loss_fn(val_t_p, val_t_y)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    train_loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    train_losses.append(train_loss)\n",
        "    val_losses.append(val_loss)\n",
        "\n",
        "    if epoch % 500 == 0:\n",
        "      print(f'Epochs: {epoch}, \\tTraining Loss: {train_loss:.6f}, \\tValidation Loss: {val_loss:.6f}\\n')\n",
        "\n",
        "  return params, train_losses, val_losses\n",
        "\n",
        "# training loop for 11 parameters\n",
        "def linear_train2(epochs, optimizer, params, train_t_x, val_t_x, train_t_y, val_t_y):\n",
        "  train_losses = []\n",
        "  val_losses = []\n",
        "\n",
        "  for epoch in range(1, epochs+1):\n",
        "    train_t_p = linear_model2(train_t_x, *params)\n",
        "    train_loss = loss_fn(train_t_p, train_t_y)\n",
        "\n",
        "    val_t_p = linear_model2(val_t_x1, *params)\n",
        "    val_loss = loss_fn(val_t_p, val_t_y)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    train_loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    train_losses.append(train_loss)\n",
        "    val_losses.append(val_loss)\n",
        "\n",
        "    if epoch % 500 == 0:\n",
        "      print(f'Epochs: {epoch}, \\tTraining Loss: {train_loss:.6f}, \\tValidation Loss: {val_loss:.6f}\\n')\n",
        "\n",
        "  return params, train_losses, val_losses"
      ],
      "metadata": {
        "id": "LQvY6p4x4CzG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features = [\"area\", \"bedrooms\", \"bathrooms\", \"stories\", \"parking\"]\n",
        "\n",
        "x = housing[features].values\n",
        "y = housing['price'].values\n",
        "\n",
        "sc = StandardScaler()\n",
        "x_sc = sc.fit_transform(x)\n",
        "\n",
        "t_x = torch.tensor(x_sc)\n",
        "t_y = torch.tensor(y)\n",
        "\n",
        "n_samples = t_x.shape[0]\n",
        "n_val = int(0.2 * n_samples)\n",
        "\n",
        "shuffled_indices = torch.randperm(n_samples)\n",
        "\n",
        "train_indices = shuffled_indices[:-n_val]\n",
        "val_indices = shuffled_indices[-n_val:]\n",
        "\n",
        "train_t_x = t_x[train_indices]\n",
        "train_t_y = t_y[train_indices]\n",
        "\n",
        "val_t_x = t_x[val_indices]\n",
        "val_t_y = t_y[val_indices]\n",
        "\n",
        "train_indices, val_indices\n",
        "train_t_x.shape\n",
        "\n"
      ],
      "metadata": {
        "id": "cp-boFTt8dPp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Adam with different learning rates\n",
        "params = torch.tensor([1.0, 1.0, 1.0, 1.0, 1.0, 0.0], requires_grad=True)\n",
        "optimizer = optim.Adam([params], lr=0.1)\n",
        "print(f'Learning rate: 0.1\\n')\n",
        "trained_params, train_losses, val_losses = linear_train1(\n",
        "    5000, optimizer, params, train_t_x, val_t_x, train_t_y, val_t_y\n",
        ")\n",
        "\n",
        "params = torch.tensor([1.0, 1.0, 1.0, 1.0, 1.0, 0.0], requires_grad=True)\n",
        "optimizer = optim.Adam([params], lr=0.01)\n",
        "print(f'Learning rate: 0.01\\n')\n",
        "trained_params1, train_losses1, val_losses1 = linear_train1(\n",
        "    5000, optimizer, params, train_t_x, val_t_x, train_t_y, val_t_y\n",
        ")\n",
        "\n",
        "params = torch.tensor([1.0, 1.0, 1.0, 1.0, 1.0, 0.0], requires_grad=True)\n",
        "optimizer = optim.Adam([params], lr=0.001)\n",
        "print(f'Learning rate: 0.001\\n')\n",
        "trained_params2, train_losses2, val_losses2 = linear_train1(\n",
        "    5000, optimizer, params, train_t_x, val_t_x, train_t_y, val_t_y\n",
        ")\n",
        "\n",
        "params = torch.tensor([1.0, 1.0, 1.0, 1.0, 1.0, 0.0], requires_grad=True)\n",
        "optimizer = optim.Adam([params], lr=0.0001)\n",
        "print(f'Learning rate: 0.0001\\n')\n",
        "trained_params3, train_losses3, val_losses3 = linear_train1(\n",
        "    5000, optimizer, params, train_t_x, val_t_x, train_t_y, val_t_y\n",
        ")\n",
        "\n",
        "# SGD with different learning rates\n",
        "params = torch.tensor([1.0, 1.0, 1.0, 1.0, 1.0, 0.0], requires_grad=True)\n",
        "optimizer = optim.SGD([params], lr=0.1)\n",
        "print(f'Learning rate: 0.1\\n')\n",
        "trained_params4, train_losses4, val_losses4 = linear_train1(\n",
        "    5000, optimizer, params, train_t_x, val_t_x, train_t_y, val_t_y\n",
        ")\n",
        "params = torch.tensor([1.0, 1.0, 1.0, 1.0, 1.0, 0.0], requires_grad=True)\n",
        "optimizer = optim.SGD([params], lr=0.1)\n",
        "print(f'Learning rate: 0.01\\n')\n",
        "trained_params5, train_losses5, val_losses5 = linear_train1(\n",
        "    5000, optimizer, params, train_t_x, val_t_x, train_t_y, val_t_y\n",
        ")\n",
        "params = torch.tensor([1.0, 1.0, 1.0, 1.0, 1.0, 0.0], requires_grad=True)\n",
        "optimizer = optim.SGD([params], lr=0.1)\n",
        "print(f'Learning rate: 0.001\\n')\n",
        "trained_params6, train_losses6, val_losses6 = linear_train1(\n",
        "    5000, optimizer, params, train_t_x, val_t_x, train_t_y, val_t_y\n",
        ")\n",
        "params = torch.tensor([1.0, 1.0, 1.0, 1.0, 1.0, 0.0], requires_grad=True)\n",
        "optimizer = optim.SGD([params], lr=0.1)\n",
        "print(f'Learning rate: 0.0001\\n')\n",
        "trained_params7, train_losses7, val_losses7 = linear_train1(\n",
        "    5000, optimizer, params, train_t_x, val_t_x, train_t_y, val_t_y\n",
        ")"
      ],
      "metadata": {
        "id": "DckXkQVV-T46"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# List of variables to map\n",
        "varlist = ['mainroad', 'guestroom', 'basement', 'hotwaterheating','airconditioning', 'prefarea']\n",
        "\n",
        "# Defining the map function\n",
        "def binary_map(x):\n",
        "  return x.map({'yes': 1, 'no': 0})\n",
        "\n",
        "# Applying the function to the list\n",
        "housing[varlist] = housing[varlist].apply(binary_map)\n",
        "housing.head()\n",
        ""
      ],
      "metadata": {
        "id": "CQPvlbkgWrwd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feature_list = ['area', 'bedrooms', 'stories', 'mainroad', 'guestroom', 'basement', 'hotwaterheating', 'airconditioning', 'parking', 'prefarea']\n",
        "x1 = housing[feature_list].values\n",
        "y1 = housing['price'].values.reshape(-1,1)\n",
        "\n",
        "sc = StandardScaler()\n",
        "x1_sc = sc.fit_transform(x1)\n",
        "\n",
        "t_x1 = torch.tensor(x1_sc)\n",
        "t_y1 = torch.tensor(y1)\n",
        "\n",
        "n_samples = t_x1.shape[0]\n",
        "n_val = int(0.2 * n_samples)\n",
        "\n",
        "shuffled_indices = torch.randperm(n_samples)\n",
        "\n",
        "train_indices = shuffled_indices[:-n_val]\n",
        "val_indices = shuffled_indices[-n_val:]\n",
        "\n",
        "train_t_x1 = t_x1[train_indices]\n",
        "train_t_y1 = t_y1[train_indices]\n",
        "\n",
        "val_t_x1 = t_x1[val_indices]\n",
        "val_t_y1 = t_y1[val_indices]\n",
        "\n",
        "train_t_x1.shape"
      ],
      "metadata": {
        "id": "IThCL9TRlmF8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Adam with different learning rates\n",
        "params = torch.tensor([1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0], requires_grad=True)\n",
        "optimizer = optim.Adam([params], lr=0.1)\n",
        "print(f'Learning rate: 0.1\\n')\n",
        "trained_params8, train_losses8, val_losses8 = linear_train2(\n",
        "    5000, optimizer, params, train_t_x1, val_t_x1, train_t_y1, val_t_y1\n",
        ")\n",
        "\n",
        "params = torch.tensor([1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0], requires_grad=True)\n",
        "optimizer = optim.Adam([params], lr=0.01)\n",
        "print(f'Learning rate: 0.01\\n')\n",
        "trained_params9, train_losses9, val_losses9 = linear_train2(\n",
        "    5000, optimizer, params, train_t_x1, val_t_x1, train_t_y1, val_t_y1\n",
        ")\n",
        "\n",
        "params = torch.tensor([1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0], requires_grad=True)\n",
        "optimizer = optim.Adam([params], lr=0.001)\n",
        "print(f'Learning rate: 0.001\\n')\n",
        "trained_params10, train_losses10, val_losses10 = linear_train2(\n",
        "    5000, optimizer, params, train_t_x1, val_t_x1, train_t_y1, val_t_y1\n",
        ")\n",
        "\n",
        "params = torch.tensor([1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0], requires_grad=True)\n",
        "optimizer = optim.Adam([params], lr=0.0001)\n",
        "print(f'Learning rate: 0.0001\\n')\n",
        "trained_params11, train_losses11, val_losses11 = linear_train2(\n",
        "    5000, optimizer, params, train_t_x1, val_t_x1, train_t_y1, val_t_y1\n",
        ")\n",
        "\n",
        "# SGD with different learning rates\n",
        "params = torch.tensor([1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0], requires_grad=True)\n",
        "optimizer = optim.SGD([params], lr=0.1)\n",
        "print(f'Learning rate: 0.1\\n')\n",
        "trained_params12, train_losses12, val_losses12 = linear_train2(\n",
        "    5000, optimizer, params, train_t_x1, val_t_x1, train_t_y1, val_t_y1\n",
        ")\n",
        "params = torch.tensor([1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0], requires_grad=True)\n",
        "optimizer = optim.SGD([params], lr=0.1)\n",
        "print(f'Learning rate: 0.01\\n')\n",
        "trained_params13, train_losses13, val_losses13 = linear_train2(\n",
        "    5000, optimizer, params, train_t_x1, val_t_x1, train_t_y1, val_t_y1\n",
        ")\n",
        "params = torch.tensor([11.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0], requires_grad=True)\n",
        "optimizer = optim.SGD([params], lr=0.1)\n",
        "print(f'Learning rate: 0.001\\n')\n",
        "trained_params14, train_losses14, val_losses14 = linear_train2(\n",
        "    5000, optimizer, params, train_t_x1, val_t_x1, train_t_y1, val_t_y1\n",
        ")\n",
        "params = torch.tensor([1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0], requires_grad=True)\n",
        "optimizer = optim.SGD([params], lr=0.1)\n",
        "print(f'Learning rate: 0.0001\\n')\n",
        "trained_params15, train_losses15, val_losses15 = linear_train2(\n",
        "    5000, optimizer, params, train_t_x1, val_t_x1, train_t_y1, val_t_y1\n",
        ")"
      ],
      "metadata": {
        "id": "Z70x3FAfmemj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "3OmpZwRpjw9S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "rwmyl-ENNav-"
      }
    }
  ]
}